{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性代数基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标量、向量、矩阵和Tensor张量\n",
    "\n",
    "* Tensor\n",
    "\n",
    "* 转换Tensor成为一个矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：标量，向量，矩阵和Tensor之间的关系？\n",
    "# 标量：一个常数，表示的是向量的长度\n",
    "# 向量：带有方向性，并有大小，在表格数据中，表示的一行数据内容\n",
    "# 矩阵：多个向量的线性组合，一般表示的是一个二维表格数据，行、列数据\n",
    "# 张量：多个矩阵的线性组合，可以由多个数轴组成，每个数轴是一个矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "<class 'numpy.matrixlib.defmatrix.matrix'> (1, 3)\n",
      "<class 'numpy.matrixlib.defmatrix.matrix'> (2, 3)\n",
      "<class 'numpy.ndarray'> (2, 2, 3)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'> (3, 2, 3)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'> (2, 3, 2, 3)\n",
      "(10, 1)\n",
      "(10, 1)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "# 标量、向量、矩阵和张量\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "scaler = 1\n",
    "vector = np.mat([1,2,3])\n",
    "matrix = np.mat([[1,2,3],[4,5,6]])\n",
    "matrix_1 = np.array([[[1,2,3],[4,5,6]],[[1,2,3],[4,5,6]]])\n",
    "matrix_2 = np.array([[[1,2,3],[4,5,6]],[[1,2,3],[4,5,6]],[[1,2,3],[4,5,6]]])\n",
    "matrix_3 = np.mat([[1,2,3],[4,5,6],[1,2,3],[4,5,6]])\n",
    "tensor = tf.convert_to_tensor(matrix_2)\n",
    "tensor_1 = tf.constant([[[[1,2,3],[4,5,6]],[[1,2,3],[4,5,6]],[[1,2,3],[4,5,6]]],[[[1,2,3],[4,5,6]],[[1,2,3],[4,5,6]],[[1,2,3],[4,5,6]]]])\n",
    "\n",
    "# print(scaler)\n",
    "# print(vector)\n",
    "# print(vector.T)\n",
    "# print(matrix)\n",
    "# print(matrix.T)\n",
    "# print(matrix_1[0])\n",
    "# print(matrix_1.T)\n",
    "# print(matrix_3)\n",
    "# with tf.Session() as sess:\n",
    "#    print(sess.run(tensor))\n",
    "#    print(sess.run(tensor_1))\n",
    "print(type(scaler)) # 常数\n",
    "print(type(vector),vector.shape) # 一维向量\n",
    "print(type(matrix),matrix.shape) # 二维矩阵 矩阵是二维\n",
    "print(type(matrix_1),matrix_1.shape) # 多维数组\n",
    "print(type(tensor),tensor.get_shape()) # 三维张量\n",
    "print(type(tensor_1),tensor_1.get_shape())\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(np.random.rand(10,4),columns=list('abcd'))\n",
    "print(df['a'][:,np.newaxis].shape)\n",
    "print(df['a'].values.reshape(10,1).shape)\n",
    "print(df['a'].as_matrix().reshape(10,1).shape)\n",
    "# 说明上述三种方法都可以将数组数据转换为矩阵数据形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一些总结：\n",
    "# pd.Series对应的是表格中的一列\n",
    "# pd.DataFrame对应的是表格中的数据\n",
    "# .shape是表述的表格的数据形状，几行，几列数据\n",
    "# .ndim是数组的维度，表格中的数据是二维或一维的\n",
    "# numpy,pandas创建的数据都是数组数据，可以采用.values.reshape()、[:np.newaxis]和.as_matrix().reshape()转换为矩阵数据\n",
    "# np.array可以创建多维数据，或者说高维数组\n",
    "# np.mat 是用于创建矩阵的 因矩阵是二维的\n",
    "# np.matrix 必须是2维\n",
    "# 多维数据是以矩阵与向量的点乘来表示的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性代数正确打开方式\n",
    "\n",
    "假设$A∈\\mathbb{R}^{m \\times n}$\n",
    "\n",
    "$$\\underbrace{\\left[\n",
    "\\begin{matrix}\n",
    "    2 & -1 \\\\\n",
    "    1 & 1\n",
    "\\end{matrix}\n",
    "\\right]}_{\\mathbf{A}∈\\mathbb{R}^{2\\times2}}\n",
    "\\underbrace{\\left[\n",
    "\\begin{matrix}\n",
    "    x \\\\\n",
    "    y\n",
    "\\end{matrix}\n",
    "\\right]}_{\\mathbf{x}∈\\mathbb{R}^{2}} = \n",
    "\\underbrace{\\left[\n",
    "\\begin{matrix}\n",
    "    1 \\\\\n",
    "    5\n",
    "\\end{matrix}\n",
    "\\right]}_{\\mathbf{b}∈\\mathbb{R}^{2}}$$\n",
    "$$\\underbrace{\\left[\n",
    "\\begin{matrix}\n",
    "    2 & 1 & 1 \\\\\n",
    "    4 & -6 & 0 \\\\\n",
    "    -2 & 7 & 2\n",
    "\\end{matrix}\n",
    "\\right]}_{\\mathbf{A}∈\\mathbb{R}^{3\\times3}}\n",
    "\\underbrace{\\left[\n",
    "\\begin{matrix}\n",
    "    u \\\\\n",
    "    v \\\\\n",
    "    w\n",
    "\\end{matrix}\n",
    "\\right]}_{\\mathbf{x}∈\\mathbb{R}^{3}} = \n",
    "\\underbrace{\\left[\n",
    "\\begin{matrix}\n",
    "    5 \\\\\n",
    "    -2 \\\\\n",
    "    9\n",
    "\\end{matrix}\n",
    "\\right]}_{\\mathbf{b}∈\\mathbb{R}^{3}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Ax = b$的行视图\n",
    "\n",
    "* 行视图-凸优化中的超平面\n",
    "\n",
    "$$\\begin{align*}\n",
    "    2x-y=1 \\\\\n",
    "    x+y=5\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ax=b的列视图\n",
    "\n",
    "* 列视图-矩阵列的线性组合。思考：所有的线性组合是？列空间\n",
    "\n",
    "$$x\\left[\n",
    "\\begin{matrix}\n",
    "    2 \\\\\n",
    "    1\n",
    "\\end{matrix}\n",
    "\\right]+\n",
    "y\\left[\n",
    "\\begin{matrix}\n",
    "    -1 \\\\\n",
    "    1\n",
    "\\end{matrix}\n",
    "\\right]=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    1 \\\\\n",
    "    5\n",
    "\\end{matrix}\n",
    "\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性相关和线性无关\n",
    "\n",
    "* 线性相关：矢量集$[\\mathbf{a_1},\\dots,\\mathbf{a_n}]$是线性相关的，如果$\\sum_{k=1}^nc_k\\mathbf{a}_k=0$，当且仅当$c_1,c_2,\\dots,c_n≠0$。即，至少有一个向量，$\\mathbf{a_1}$可以由其它向量线性表示：\n",
    "\n",
    "$$\\mathbf{a}_1=-\\frac{1}{c_1}\\sum_{k=2}^nc_k\\mathbf{a}_k$$\n",
    "\n",
    "* 线性无关：矢量集$[\\mathbf{a_1},\\dots,\\mathbf{a_n}]$是线性无关的(即不是线性相关的)，如果$\\sum_{k=1}^nc_k\\mathbf{a}_k=0$，当且仅当$c_1,c_2,\\dots,c_n=0$\n",
    "\n",
    "\n",
    "* 定义A=$[\\mathbf{a_1},\\dots,\\mathbf{a_n}]$，则$\\mathbf{Ax=0}$只有$\\mathbf{x}=0$，没有其它的线性组合能产生0，此时矩阵A可逆\n",
    "\n",
    "    * 证明：由$\\mathbf{Ax=0}$ $\\Rightarrow$ $\\sum_{i=1}^nx_i\\mathbf{a}_i=0$ & $\\mathbf{x}=0$ $\\Rightarrow$ 矢量集$[\\mathbf{a_1},\\dots,\\mathbf{a_n}]$线性无关 $\\Rightarrow$ 矩阵A的行列式不等于0 $\\Rightarrow$ 矩阵A可逆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当矩阵A中的所有列向量线性无关时，Ax=b的解只为列空间构成的通解，不存在零空间的特解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性相关和线性无关(举例)\n",
    "\n",
    "举例\n",
    "\n",
    "$\\mathbf{A}=[\\mathbf{a}_1,\\mathbf{a}_2,\\mathbf{a}_3]=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    1 & 2 & 1 \\\\\n",
    "    0 & 3 & -1 \\\\\n",
    "    0 & 0 & 1\n",
    "\\end{matrix}\n",
    "\\right]$ 线性无关，why？\n",
    "\n",
    "$\\mathbf{B}=[\\mathbf{b}_1,\\mathbf{b}_2,\\mathbf{b}_3]=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    1 & 2 & -3 \\\\\n",
    "    0 & 3 & -3 \\\\\n",
    "    1 & 1 & -2\n",
    "\\end{matrix}\n",
    "\\right]$ 线性相关，why？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Span、基和子空间\n",
    "\n",
    "* Span：\n",
    "\n",
    "$$span\\hspace{0.1cm}[\\mathbf{a_1},\\dots,\\mathbf{a_n}]=\\left\\{\\mathbf{y}∈\\mathbb{R}^m\\hspace{0.1cm}|\\hspace{0.1cm}\\mathbf{y}=\\sum_{k=1}^nc_k\\mathbf{a}_k\\right\\} = S$$\n",
    "\n",
    "* $\\mathbf{A}=[\\mathbf{a_1},\\dots,\\mathbf{a_n}]$的所有线性组合。此时，如果$[\\mathbf{a_1},\\dots,\\mathbf{a_n}]$线性无关的，则他们是$S$的一组基。正交基是指$\\mathbf{a_i^Ta_j}=0$\n",
    "\n",
    "* $S$可以有不同的一组基，但是基里向量的个数是相同的，被称为$S$的维数，等于rank($A$)。一个子空间用一组基就可以表示了！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{A}=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    1 & 2 & 3 \\\\\n",
    "    0 & 3 & 3 \\\\\n",
    "    0 & 0 & 0\n",
    "\\end{matrix}\n",
    "\\right]$，$S = span\\left[\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "    0\n",
    "\\end{matrix}\n",
    "\\right],\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    2 \\\\\n",
    "    3 \\\\\n",
    "    0\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\right]$，思考还有其它基吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性无关的一组向量张成的空间是矩阵的列空间，也是矩阵的一组基，矩阵的基可以有多个，但是基里包含的向量个数相同\n",
    "# 正交一定线性无关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性代数精华"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "四个基本的子空间 - 列空间\n",
    "\n",
    "* 考虑$\\mathbf{A}$：列空间(值域、Span)：C($\\mathbf{A}$)是$\\mathbb{R}^m$(not $\\mathbb{R}^n$)的子空间\n",
    "\n",
    "* 定义：包含所有列的线性组合，即$C(\\mathbf{A})=\\{y=\\mathbf{Ax},\\mathbf{x}∈\\mathbb{R}^n\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{A}=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    1 & 0 \\\\\n",
    "    4 & 3 \\\\\n",
    "    2 & 3\n",
    "\\end{matrix}\n",
    "\\right]$，$C(\\mathbf{A}) = span\\left[\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    1 \\\\\n",
    "    4 \\\\\n",
    "    2\n",
    "\\end{matrix}\n",
    "\\right],\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    0 \\\\\n",
    "    3 \\\\\n",
    "    3\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\right]$，构成了一个$\\mathbb{R}^3$的子空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：列空间 - 一般找一组线性无关的向量构成，也就是它的一组基，当然非线性组也可以组成\n",
    "# 对于Ax=b，b必定属于C(A)列空间，且是列向量的线性组合得到，那可想而知Ax=0，是零空间内所有行向量的线性组合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "四个基本的子空间 - 零空间\n",
    "\n",
    "* 考虑$\\mathbf{A}$：零空间：N($\\mathbf{A}$)是$\\mathbb{R}^n$(not $\\mathbb{R}^m$)的子空间\n",
    "\n",
    "* 定义：N($\\mathbf{A}$)包含$\\mathbf{Ax=0}$的所有的解得集合\n",
    "\n",
    "* 注意：$\\mathbf{Ax}=b$的解并不形成一个子空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：要搞清楚Ax=b的解与构成子空间是不一样的概念，\n",
    "# Ax=b的解：是列空间和零空间的组合，两者并不构成子空间\n",
    "# 而构成子空间的有两对：1、列空间与左零空间构成R^m子空间；2、行空间和零空间构成R^n子空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求$\\mathbf{A}$的零空间的基\n",
    "\n",
    "$\\mathbf{A}=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    1 & 2 & 2 & 4 \\\\\n",
    "    3 & 8 & 6 & 16\n",
    "\\end{matrix}\n",
    "\\right]$ $\\to$ \n",
    "$\\mathbf{U}=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    1 & 2 & 2 & 4 \\\\\n",
    "    0 & 2 & 0 & 4\n",
    "\\end{matrix}\n",
    "\\right]$\n",
    "\n",
    "即$\\mathbf{Ux=0}$，于是有\n",
    "$S_1 = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    -2 \\\\\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "    0\n",
    "\\end{matrix}\n",
    "\\right]$\n",
    "$S_2 = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    0 \\\\\n",
    "    -2 \\\\\n",
    "    0 \\\\\n",
    "    1\n",
    "\\end{matrix}\n",
    "\\right]$\n",
    "\n",
    "因此$N(\\mathbf{A})=\n",
    "C\\left(\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    -2 \\\\\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "    0\n",
    "\\end{matrix}\n",
    "\\right],\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    0 \\\\\n",
    "    -2 \\\\\n",
    "    0 \\\\\n",
    "    1\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\right)$，是$\\mathbb{R}^4$的子空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：零空间 - 一般是由非主元列构成，也就是n-r，r为矩阵A的秩\n",
    "# Ax=b为列空间所有解得线性组合，而Ax=0是零空间的所有线性组合，也是线性无关的列空间一组解+零空间的一组解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "四个基本的子空间 - 行空间\n",
    "\n",
    "* 考虑c：行空间：C($\\mathbf{A}^T$)是$\\mathbb{R}^n$的子空间\n",
    "\n",
    "* 定义：包含所有行的线性组合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{A}=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    1 & 2 & 2 & 4 \\\\\n",
    "    3 & 8 & 6 & 16\n",
    "\\end{matrix}\n",
    "\\right]$，\n",
    "则$C(\\mathbf{A}^T$)=\n",
    "$C\\left(\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    1 \\\\\n",
    "    2 \\\\\n",
    "    2 \\\\\n",
    "    4\n",
    "\\end{matrix}\n",
    "\\right],\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    3 \\\\\n",
    "    8 \\\\\n",
    "    6 \\\\\n",
    "    16\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "四个基本的子空间 - 左零空间\n",
    "\n",
    "* 考虑$\\mathbf{A}^T$：左零空间：$N(\\mathbf{A}^T)=\\{\\mathbf{A}^Ty=0\\}$的解集合，是$\\mathbb{R}^m$的子空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 四个基本的子空间的关系\n",
    "# 列空间：是R^m的子空间 C(A)={Ax=b}的解 其秩为r\n",
    "# 零空间：是R^n的子空间 N(A)={Ax=0}的解 其秩为n-r\n",
    "# 行空间：是R^n的子空间 C(A^T)={A^Tx=b}的解 其秩为r\n",
    "# 左零空间：是R^m的子空间 N(A^T)={A^Ty=0}的解 其秩为m-r\n",
    "# 列空间和左零空间构成所有R^n空间 互为正交补\n",
    "# 行空间和零空间构成所有R^m空间 互为正交补"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用子空间重新看待线性方程组的解\n",
    "\n",
    "Ax=b方程的解\n",
    "\n",
    "* 只有唯一解，则$b∈C(A)$，$N(A)$的维数是0\n",
    "* 有无穷多个解，$b∈C(A)$，$N(A)$的维数大于0\n",
    "* 无解，则$b∉C(A)$\n",
    "* 如果有解，解得形式 $x=p+v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：\n",
    "# 1、只有唯一的一个解使得Ax=b，也就是所有向量的线性组合构成Ax=b的解，没有零空间的成分\n",
    "# 2、有无穷多个解，说明存在零空间的成分构成Ax=b的解，这个特解有多个\n",
    "# 3、无解，自然是b∉C(A)\n",
    "# 4、有解得形式是通解(来自列空间)+特解(来自零空间)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可逆矩阵\n",
    "\n",
    "* 奇异/非奇异：$A∈\\mathbb{R}^{m \\times m}$是非奇异\n",
    "\n",
    "$$Ax=0 \\Longleftrightarrow x=0$$\n",
    "\n",
    "* 非奇异矩阵：$[\\mathbf{a_1},\\dots,\\mathbf{a_n}]$线性无关\n",
    "\n",
    "* 非奇异 = 可逆\n",
    "\n",
    "* $AA^{-1} = I$\n",
    "\n",
    "* $(AB)^{-1} = B^{-1}A{-1}$\n",
    "\n",
    "* $(A^{-1})^T = (A^T)^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：非奇异矩阵 <=> 矩阵可逆 => 行列式不为0 => Ax=0 => x=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方阵的特征值与特征向量\n",
    "\n",
    "Ax=λx几何意义，并思考如何计算$A^{1000}$\n",
    "\n",
    "给定一个矩阵$A=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    4 & 1 \\\\\n",
    "    1 & 4\n",
    "\\end{matrix}\n",
    "\\right]$，对于$x_1=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    1 \\\\\n",
    "    0\n",
    "\\end{matrix}\n",
    "\\right]$，则有$Ax_1=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    4 \\\\\n",
    "    1\n",
    "\\end{matrix}\n",
    "\\right]$；对于$x_3=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    1 \\\\\n",
    "    1\n",
    "\\end{matrix}\n",
    "\\right]$，则有$Ax_3=\n",
    "5\\left[\n",
    "\\begin{matrix}\n",
    "    1 \\\\\n",
    "    1\n",
    "\\end{matrix}\n",
    "\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：对于一个方阵A，必然存在一个特征值λ和特征向量x，使得Ax=λx\n",
    "# 其几何意义：是方阵A在向量x的作用下进行了旋转，变成了一个向量，并伸缩了λ倍\n",
    "# 物理意义：特征值 - 速度变化的大小；特征向量：速度变化的方向\n",
    "# 很明显存在多组特征值和特征向量使得等式成立，也肯定存在最大的特征值和对应的特征向量，因特征值是实数，这也为PCA取主成分提供了依据\n",
    "# 这种矩阵的变换，也能够为矩阵计算简化计算量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征分解的性质\n",
    "\n",
    "特征分解的一般性质\n",
    "\n",
    "* Ax=λx，λ为特征值，x为特征向量求取特征值(A-λI)x=0，det(A-λI)=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：因要等式有意义，特征向量x不能为0向量，所以只能A-λI为0矩阵，继而其行列式det(A-λI)=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 对于$Ax_i=λx_i$，如果所有的特征值都不相同，则相应的所有的特征向量线性无关。此时，A可以被对角化为\n",
    "\n",
    "$$A=V \\varLambda V^{-1}$$\n",
    "\n",
    "其中$V=[x_1,\\dots,x_n]$，$\\varLambda = Diag(λ_1,\\dots,λ_n)$，思考$A^{1000}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：[x1,…,xn]特征向量要线性无关，假设c1x1+...+cnxn=0成立，等式两边同乘矩阵A，\n",
    "# 得c1Ax1+...+cnAxn=0，根据行列式det(A-λI)=0，有c1λ1x1+...+cnλnxn=0，用该式减去λk倍的\n",
    "# (c1x1+...+cnxn)，得c1(λ1-λk)x1+...+cn(λn-λk)xn=0，因所有特征值都不相同，继而λi-λk≠0\n",
    "# 所以得cixi=0，继而得正c1x1+...+cnxn=0，[x1,…,xn]特征向量要线性无关\n",
    "# 存在多组这样的λi和特征值向量xi，必定存在最大的λ，和其对应的特征向量x，这些特征值λ和特征向量\n",
    "# 能够变换成矩阵A，这是在研究矩阵的性质，将矩阵进行变换化简，比如存在复杂矩阵运算的情况\n",
    "\n",
    "# 如果方阵A可以被对角化，要计算A^1000，就相当于计算VΛ^1000V^-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 思考：所有的方阵都可以对角化吗？\n",
    "\n",
    "    答：不是所有的方阵都可以，特征值有重根的就不行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对称矩阵的特征分解\n",
    "\n",
    "一个对称矩阵，无论其特征值相同或者不同，则其相同的所有的特征向量正交($UU^T=U^TU=I$) 正交矩阵有$U^T=U^{-1}$，$u_i \\cdot u_j=0$，$u_i \\cdot u_i=1$\n",
    "\n",
    "$$\\begin{align*}\n",
    "A &=U \\varLambda U^T \\\\\n",
    "  &=[u_1,\\dots,u_n]\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    λ_1 & & \\\\\n",
    "     & \\ddots & \\\\\n",
    "     &  & λ_n\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    u_1^T \\\\\n",
    "    \\vdots \\\\\n",
    "    u_n^T\n",
    "\\end{matrix}\n",
    "\\right] \\\\\n",
    "&=\\sum_{i=1}^nλ_iu_iu_i^T\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对称矩阵的特征分解\n",
    "\n",
    "* 对称矩阵的特征值是实数\n",
    "\n",
    "* 如果$A∈\\mathbb{R}^{n \\times n}$是一对称矩阵且且rank $r \\le n$，则有\n",
    "\n",
    "$$\\underbrace{|λ_1| \\ge |λ_2| \\ge \\dots \\ge |λ_r|}_\\text{r} > \\underbrace{λ_{r+1}=\\dots=λ_n}_\\text{n-r}=0$$\n",
    "\n",
    "> 说明：如果矩阵$\\mathbf{A}$个列向量都线性独立，则有$r = n$，如果存在线性相关，则$r < n$\n",
    "\n",
    "* Rank($A^TA$) = Rank($AA^T$) = Rank($A$) = Rank($\\varLambda$) - 要求A是对称矩阵\n",
    "\n",
    "* 思考对于任意矩阵，能否找到一个类似的分解?\n",
    "\n",
    "    任意矩阵是奇异值分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征分解和子空间的关系\n",
    "\n",
    "* 列空间：$C(\\mathbf{A}) = \\mathbf{\\{y\\hspace{0.1cm}|\\hspace{0.1cm}y=Ax\\}}$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbf{Ax}&=[\\mathbf{U_1U_2}]\n",
    "    \\left[\n",
    "    \\begin{matrix}\n",
    "        \\varLambda_1 & 0 \\\\\n",
    "        0 & \\varLambda_2\n",
    "    \\end{matrix}\n",
    "    \\right]\n",
    "    \\left[\n",
    "    \\begin{matrix}\n",
    "        \\mathbf{U_1}^T \\\\\n",
    "        \\mathbf{U_2}^T\n",
    "    \\end{matrix}\n",
    "    \\right]\\mathbf{x} \\\\\n",
    "    &=[\\mathbf{U_1U_2}]\n",
    "    \\left[\n",
    "    \\begin{matrix}\n",
    "        \\varLambda_1 & 0 \\\\\n",
    "        0 & \\varLambda_2\n",
    "    \\end{matrix}\n",
    "    \\right]\n",
    "    \\left[\n",
    "    \\begin{matrix}\n",
    "        \\mathbf{c_1} \\\\\n",
    "        \\mathbf{c_2}\n",
    "    \\end{matrix}\n",
    "    \\right] \\\\\n",
    "    &=[\\mathbf{U_1U_2}]\n",
    "    \\left[\n",
    "    \\begin{matrix}\n",
    "        \\varLambda_1\\mathbf{c_1} \\\\\n",
    "        \\varLambda_2\\mathbf{c_2} \n",
    "    \\end{matrix}\n",
    "    \\right] \\\\\n",
    "    &=\\mathbf{U_1}(\\varLambda_1\\mathbf{c_1})+\\mathbf{U_2}(\\varLambda_2\\mathbf{c_2})\n",
    "\\end{align*}$$\n",
    "\n",
    "* 因$\\mathbf{\\varLambda_2=0}$，得$C(\\mathbf{A})=C(\\mathbf{U_1})$\n",
    "\n",
    "* 零空间：$N(\\mathbf{A})=\\{\\mathbf{x≠0}\\hspace{0.1cm}|\\hspace{0.1cm}\\mathbf{Ax}=0\\}$，同理可得$\\mathbf{U}_2$是$N(\\mathbf{A})$的正交基"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：从上式推导中可以看出，y=Ax的解，一部分来自列空间C(A)，另一部分来自零空间N(A)\n",
    "# 由此也可以进一步说明，C(A)=C(U1)，U2是N(A)的正交基\n",
    "# Rank(A)=Rank(U1)=Rank(Λ1)\n",
    "# 令y=Λ1c1，很明显可以看出Ax=U1y，y向量的各个元素是关于U1的列向量的线性组合，即有C(A)=C(U1)\n",
    "# 实数可以排序，确定了特征值可以比较大小，继而排序，并筛选较优特征值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化问题\n",
    "\n",
    "* 约束问题\n",
    "\n",
    "$$\\max \\mathbf{x^TAx}\\hspace{0.3cm}subject\\hspace{0.1cm}to\\hspace{0.1cm}\\|\\mathbf{x}\\|_2^2=1$$\n",
    "\n",
    "* 拉格朗日函数\n",
    "\n",
    "$$L(\\mathbf{x},λ)=\\mathbf{x^TAx}-λ\\mathbf{x^Tx}$$\n",
    "\n",
    "* 计算梯度\n",
    "\n",
    "$$\\nabla L(\\mathbf{x},λ)=2\\mathbf{Ax}-2λ\\mathbf{x}=\\mathbf{0}$$\n",
    "\n",
    "* 可得\n",
    "\n",
    "$$\\mathbf{Ax=λx}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A表示的是一数据矩阵，如将进行降维的数据，或者是图像数据矩阵\n",
    "# x是特征向量\n",
    "# xTAx是降维后的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由Ax=λx代入，得x^TAx=x^Tλx=λx^Tx=λ，由此可说明最大化x^TAx，其实是在找最大的特征值λ\n",
    "# 说明：x^TAx中的x是特征向量，A是要降维的数据，∥x∥22=1条件为1是因为矩阵对角化要求特征向量是要经过正交和单位化的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征值的应用：图像压缩，数据降维等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA\n",
    "\n",
    "正交变换\n",
    "\n",
    "* 维数灾难 - 一般说是高维特征数据存在计算量巨大的情况\n",
    "* 通过数学变换将原始高维属性空间转变成一低维空间(跟特征映射低维到高维相反)\n",
    "* 给定d维空间的样本$\\mathbf{X}=[\\mathbf{x_1},\\mathbf{x_2},\\dots,\\mathbf{x_N}]∈\\mathbb{R}^{d \\times N}$，变换之后得到$d' \\le d$维空间中的样本\n",
    "$$\\mathbf{Z=W^TX}$$\n",
    "其中$\\mathbf{W}∈\\mathbb{R}^{d \\times d'}$是变换矩阵，$\\mathbf{Z}∈\\mathbb{R}^{d' \\times N}$是样本在新空间中的表达\n",
    "* 投影解释：$\\mathbf{W}=[\\mathbf{w_1},\\mathbf{w_2},\\dots,\\mathbf{w}_{d'}]$：d'个d维向量，$\\mathbf{z_n=W^Tx_n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：每个样本d维特征降到d'维特征，降维后的特征并不一定能够形象表达\n",
    "# 整个数据降维的过程可以用投影的理论来表达，从高维沿着某个坐标系将数据垂直映射下去，需要保证投影后的数据分散足够大，也就是协方差矩阵足够大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 假定d'=1，$\\mathbf{w_1^Tw_1}=1$。每个数据点投影值为$\\mathbf{w_1^Tx_n}$，样本均值为$\\mathbf{\\bar{x}}=\\frac{1}{N}\\sum_{n=1}^{N}\\mathbf{x_n}$，投影数据均值是$\\frac{1}{N}\\sum_{n=1}^N\\mathbf{w_1^Tx_n}$。投影数据的方差为$\\frac{1}{N}\\sum_{n=1}^N\\{\\mathbf{w_1^Tx_n-w_1^T\\bar{x}}\\}^2=\\mathbf{w_1^TC_xw_1}$\n",
    "\n",
    "* 其中协方差矩阵定义为$\\mathbf{C_x}=\\frac{1}{N}\\sum_{n=1}^N(\\mathbf{x_n-\\bar{x}})(\\mathbf{x_n-\\bar{x}})^T$\n",
    "优化目标为\n",
    "$$\\max\\hspace{0.2cm}\\mathbf{w_1^TC_xw_1}\\hspace{0.2cm}subject\\hspace{0.1cm}to\\hspace{0.1cm}\\|\\mathbf{w_1}\\|_2^2=1$$\n",
    "\n",
    "* 特征向量被称为第一主成分。d' > 1数学归纳法类推。选取准则：$\\frac{\\sum_{i=1}^{d'}λ_i}{\\sum_{i=1}^dλ_i} \\ge t$，例如$t=95\\%$ \n",
    "\n",
    "> 说明：这里的损失函数是以原数据均值和投影后的数据均值的误差为目的的优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据降维我们优化的最终目标是最大化特征值，而这边的协方差矩阵CX就是之前矩阵A，对于任意特征值，对称矩阵A都存在特征值分解，故而这边的协方差矩阵\n",
    "# 也是对称矩阵，所以首先要先计算出协方差矩阵，然后再优化协方差矩阵的特征值和对应的特征向量，最后按照标准取其主成分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：我们的优化目标是使得降维后的数据尽量分散，如何衡量数据的分散程度，协方差矩阵能够表现这一属性特性\n",
    "# 对于降维后的主成分选取，不同的行业可能会有指标要求，一般是要求大于85%\n",
    "# 为什么要找垂直投影到新的坐标系？这样才能保证投影后的数据尽量分散\n",
    "# 从投影的角度，投影坐标系有很多个，怎样能够使得投影后的数据最大化分散，也就是要求，所有数据点垂直于投影坐标系，这也就对应了找最大化特征值方向的特征向量\n",
    "# 因特征向量都是正交的，即所有的投影坐标系都是正交出现的，那也肯定存在最大化投影的坐标系\n",
    "# 最大化特征值，也对应了最大化的拉伸长度，也就是数据越分散"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于图像压缩来说，我们肯定选择奇异值分解，而不是严格要求矩阵是对称矩阵的特征值\n",
    "# 而数据降维，我们用的就是特征值分解，因任意样本数据的协方差矩阵是对称半正定矩阵，故而可以求出协方差矩阵的特征值和特征向量，继而达到降维的目的\n",
    "# 对于特征分解，我们要求优化矩阵是对称矩阵\n",
    "# 而对于奇异值分解，可以为任意矩阵\n",
    "# 注意协方差矩阵需要进行归一化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA举例\n",
    "\n",
    "PCA降维举例(思考特征值反应了什么？)\n",
    "\n",
    "1、$\\mathbf{X}=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    -1 & -1 & 0 & 2 & 0 \\\\\n",
    "    -2 & 0 & 0 & 1 & 1\n",
    "\\end{matrix}\n",
    "\\right]$，$\\mathbf{C_X}=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    \\frac{6}{5} & \\frac{4}{5}\\\\\n",
    "    \\frac{4}{5} & \\frac{6}{5}\n",
    "\\end{matrix}\n",
    "\\right]$\n",
    "\n",
    "2、计算$\\mathbf{C_X}$特征值为：$λ_1=2，λ_2=\\frac{2}{5}$，特征值特征向量为\n",
    "$\\left[\n",
    "\\begin{matrix}\n",
    "    \\frac{1}{\\sqrt{2}} \\\\\n",
    "    \\frac{1}{\\sqrt{2}}\n",
    "\\end{matrix}\n",
    "\\right]$，\n",
    "$\\left[\n",
    "\\begin{matrix}\n",
    "    -\\frac{1}{\\sqrt{2}} \\\\\n",
    "    \\frac{1}{\\sqrt{2}}\n",
    "\\end{matrix}\n",
    "\\right]$\n",
    "\n",
    "3、降维：$\\left[\\frac{1}{\\sqrt{2}}\\hspace{0.2cm}\\frac{1}{\\sqrt{2}}\\right]\\mathbf{X}$ = $\\left[-\\frac{3}{\\sqrt{2}}\\hspace{0.2cm}-\\frac{1}{\\sqrt{2}}\\hspace{0.2cm}0 \\hspace{0.2cm}\\frac{3}{\\sqrt{2}}\\hspace{0.2cm}\\frac{1}{\\sqrt{2}}\\right]$，此时可验证$\n",
    "\\mathbf{C_Y}=2=λ_1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：特征向量反应了对降维后数据影响的程度\n",
    "# 这边的协方差矩阵是一个对称正定矩阵，可以将其变换为UΣU^T，故而是在求协方差矩阵的特征值和特征向量\n",
    "# 参考：https://www.zhihu.com/question/36348219\n",
    "# https://docs.scipy.org/doc/numpy-1.9.0/reference/generated/numpy.cov.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.5 0.5] \n",
      " [[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n",
      "[2.5 0.5] \n",
      " [[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n",
      "[[-2.12132034 -0.70710678  0.          2.12132034  0.70710678]]\n",
      "2.4999999999999996\n",
      "[2.5]\n",
      "[[-0.70710678 -0.70710678]]\n",
      "1\n",
      "[0.83333333]\n",
      "[[ 2.12132034]\n",
      " [ 0.70710678]\n",
      " [ 0.        ]\n",
      " [-2.12132034]\n",
      " [-0.70710678]]\n",
      "[[-1.5 -1.5]\n",
      " [-0.5 -0.5]\n",
      " [ 0.   0. ]\n",
      " [ 1.5  1.5]\n",
      " [ 0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "# 使用python计算\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.mat([[-1,-1,0,2,0],[-2,0,0,1,1]])\n",
    "C_X = np.cov(X)\n",
    "λ,vec = np.linalg.eig(C_X)\n",
    "print(λ,'\\n',vec)\n",
    "# 说明：参数bias是控制计算时除以n-1，还是n，默认是n-1为False\n",
    "# n-1是对方差的无变差估计\n",
    "# 因我们在均值μ未知的情况下，使用X拔来替代μ时，会低估方差，所以我们需要将n变成n-1，让其值变得更大点\n",
    "# 详细解释和数学证明见：https://www.zhihu.com/question/20099757\n",
    "\n",
    "N = np.dot((X-X.mean()),(X-X.mean()).T).mean()\n",
    "C_X_1 = (np.dot((X-X.mean()),(X-X.mean()).T))/(N-1)\n",
    "C_X_1\n",
    "λ_1,vec_1 = np.linalg.eig(C_X_1)\n",
    "print(λ_1,'\\n',vec_1)\n",
    "Y = vec_1.T[0].dot(X)\n",
    "print(Y)\n",
    "print(np.cov(Y))\n",
    "# 说明上式计算中使用的N样本，而不是N-1，如果严格来说，应该使用N-1观测样本来计算样本方差\n",
    "\n",
    "# 使用sklearn对数据进行降维\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = pd.DataFrame(np.array(X.T))\n",
    "\n",
    "pca = PCA(n_components = 1)\n",
    "pca.fit(df)\n",
    "print(pca.explained_variance_) # 输出特征值\n",
    "print(pca.components_) # 输出特征向量\n",
    "print(pca.n_components_) # 输出成分的个数\n",
    "print(pca.explained_variance_ratio_) # 输出保留数据的方差占比\n",
    "print(pca.transform(df))\n",
    "print(pca.inverse_transform(pca.transform(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "证明：为什么在均值$μ$未知的情况下，使用$\\bar{X}$来替代$μ$，方差的分母为$N-1$，且这种替代的方法会低估方差\n",
    "\n",
    "在均值μ已知的情况下，随机变量$X$对应样本数据的方差为：\n",
    "\n",
    "设样本总数为$N$\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^N(X_i - μ)^2$$\n",
    "\n",
    "假设现在$μ$未知，很多情况下，我们会想到用观测到的样本均值$\\widehat{X}$去替代$μ$，这个可想而知，会存在偏差，证明如下：\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\sigma^{'2} = \\frac{1}{N}\\sum_{i=1}^N(X_i - \\bar{X})^2 &= \\frac{1}{N}\\sum_{i=1}^N\\left[(X_i - μ) + (μ - \\bar{X})\\right]^2 \\\\\n",
    "                                         &= \\frac{1}{N}\\sum_{i=1}^N\\left[(X_i - μ)^2 + 2(X_i - μ)(μ - \\bar{X}) + (μ - \\bar{X})^2\\right] \\\\\n",
    "                                         &= \\frac{1}{N}\\sum_{i=1}^N(X_i - μ)^2 + \\frac{2}{N}\\sum_{i=1}^N(X_i - μ)(μ - \\bar{X}) + (μ - \\bar{X})^2 \\\\\n",
    "                                         &= \\frac{1}{N}\\sum_{i=1}^N(X_i - μ)^2 + 2(μ - \\bar{X})(μ - \\bar{X}) + (μ - \\bar{X})^2\\\\\n",
    "                                         &= \\frac{1}{N}\\sum_{i=1}^N(X_i - μ)^2 - (μ - \\bar{X})^2 \\\\\n",
    "                                         &= \\sigma^2 - (\\bar{X} - μ)^2\n",
    "\\end{align*}$$\n",
    "\n",
    "从上述式子可以看出，确实是比实际方差小了，我们需要对其进行调整，增大采用样本估计方差的值，具体增大多少，这边就讲分母从$N$个样本修改成$N-1$个样本，这样数值会变大，但是为什么是$N-1$而不是其它，接着往下证明：\n",
    "\n",
    "因随机变量样本$X$满足正太分布$\\mathcal{N}(μ,\\sigma^2)$，则有其样本均值也应满足正太分布$\\mathcal{N}(μ,\\frac{\\sigma^2}{N})$\n",
    "\n",
    "故而上式继续有：\n",
    "\n",
    "$$\\sigma^{'2} = \\sigma^2 - \\frac{\\sigma^2}{N} = \\frac{N-1}{N}\\sigma^2$$\n",
    "\n",
    "要将$\\sigma^{'2}$的值调整为$\\sigma^2$的大小，则在上述等式两边同时乘以$\\frac{N}{N-1}$，得到\n",
    "\n",
    "$$\\frac{N}{N-1}\\sigma^{'2} = \\sigma^2$$\n",
    "\n",
    "$$\\Downarrow$$\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{N-1}\\sum_{i=1}^N(X_i - \\bar{X})^2$$\n",
    "\n",
    "\n",
    "说明：我们的目的就是要使用样本估计方差时，做到无偏差估计！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
