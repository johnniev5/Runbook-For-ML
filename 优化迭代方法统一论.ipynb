{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性回归模型\n",
    "# 机器学习：通过已知训练集样本数据，应用各种机器学习算法，训练出模型，以验证集数据检验模型的泛化能力，以期望在测试集上的预测能够好的效果\n",
    "# 一般样本数据都是以表格数据的形式呈现，包含属性头和index索引列，多个属性列构成样本的多个特征，我们把属性列刻画成一个向量X，其中每个属性\n",
    "# 即为它的分量，而样本数据的结果设定为方程的y值，即构成了关于结果y与属性X的方程组\n",
    "# 线性回归模型要求误差项满足正太分布，即可使用最大似然估计，求得损失函数\n",
    "# 如下表示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个训练样本：$\\{(x^{(i)},y^{(i)})\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练样本集：$\\{(x^{(i)},y^{(i)});i=1,2,...,N\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\{(x_1^{(i)},x_2^{(i)},y^{(i)})\\}→\\{\\boldsymbol{x}^{(i)},y^{(i)})\\},\n",
    "\\boldsymbol{x}^{(i)} = \n",
    "\\left[\n",
    "    \\begin{matrix}\n",
    "    x_1^{(i)}\\\\\n",
    "    x_2^{(i)}\n",
    "    \\end{matrix}\n",
    "\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性回归从最小均方误差的角度来思考，就是使得我们习得的模型预测出的y值要尽量与真实值接近\n",
    "# 如何学习这个模型？最大似然估计，最大后验估计，优化理论，最大熵模型等\n",
    "# 先从优化理论开始，分为：\n",
    "# 1、无约束优化\n",
    "# 2、有约束优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试图学习：$f(x) = wx + b$ 使得$f(x^{(i)})≈y^{(i)}$ - 单变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试图学习：$f(x) = \\boldsymbol{w}^Tx + b$ 使得$f(\\boldsymbol{x}^{(i)})≈y^{(i)}$ - 多变量\n",
    "其中b为常数，为截距"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 另一种数学符号和公式表达方法，暂时没研究！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "init_printing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$$\\int e^{x} \\cos{\\left (x \\right )}\\, dx = \\frac{e^{x}}{2} \\sin{\\left (x \\right )} + \\frac{e^{x}}{2} \\cos{\\left (x \\right )}$$"
      ],
      "text/plain": [
       "⌠                 x           x       \n",
       "⎮  x             ℯ ⋅sin(x)   ℯ ⋅cos(x)\n",
       "⎮ ℯ ⋅cos(x) dx = ───────── + ─────────\n",
       "⌡                    2           2    "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = symbols('x')\n",
    "a = Integral(cos(x)*exp(x), x)\n",
    "Eq(a, a.doit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1、无约束优化，又分为凸函数和非凸函数\n",
    "# 一般可以使用梯度分析和迭代法来求得最优解\n",
    "# 凸函数：可以找到全局最优解\n",
    "# 非凸函数：可以找到局部最优解\n",
    "# 对于一般的无约束优化问题："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自变量为标量和函数$f:\\mathbb{R}→\\mathbb{R}$ $$\\mbox{min $f(x)$ $x∈\\mathbb{R}$}$$\n",
    "自变量为向量的函数$f:\\mathbb{R}^n→\\mathbb{R}$ $$\\mbox{min $f(\\boldsymbol{x})$ $\\boldsymbol{x}∈\\mathbb{R}^n$}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化问题可能出现的极值点：\n",
    "# 极小值，极大值和鞍点\n",
    "# 极小值和极大值分为全局和局部\n",
    "# 鞍点的解释有多种：\n",
    "# 1、在微分方程中，一个方向稳定，另一个方向不稳定的奇点；\n",
    "# 2、在泛函中，既不是极大值，也不是极小值的临界点；\n",
    "# 3、在矩阵中，某行的最大值，在该列的最小值；\n",
    "# 4、物理上，一个方向的极大值，另一个方向的极小值\n",
    "# 如函数f(x)=x^n n=1,3,5,...这样的奇数，该函数有平稳点，但是既不是极大值，也不是极小值\n",
    "# 大部分函数都是非凸函数，只能找到局部极值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1、梯度为0法，求解\n",
    "# 梯度和Hessian矩阵\n",
    "# 很明显在高维属性的情况下，求Hessian矩阵是非常复杂的\n",
    "# 二阶导比一阶导下降的快，如何理解？\n",
    "# 如果从物理速度的角度来理解，一阶导数是瞬时速度，也就是加速度；二阶导数即是加速度的快慢程度，即是优化最大加速度，一阶导数每次迭代仅是找到当时\n",
    "# 的加速度，并不能找到全局的最优的加速度，所以说二阶导数，直接蹦着最优加速度去的在，自然下降的速度也就更快，反应的是加速度的变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一阶导数对应梯度：\n",
    "$$f'(\\boldsymbol{x}); g(\\boldsymbol{x}) = ∇f(\\boldsymbol{x}) = \n",
    "\\left[\n",
    "    \\begin{matrix}\n",
    "    \\frac{∂f(\\boldsymbol{x})}{∂x_1} \\\\\n",
    "    . \\\\\n",
    "    . \\\\\n",
    "    . \\\\\n",
    "    \\frac{∂f(\\boldsymbol{x})}{∂x_n}\n",
    "    \\end{matrix}\n",
    "\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二阶导数对应Hessian矩阵：\n",
    "$$f''(\\boldsymbol{x}); H(\\boldsymbol{x}) = ∇^2f(\\boldsymbol{x}) = \n",
    "\\left[\n",
    "    \\begin{matrix}\n",
    "    \\frac{∂^2f(\\boldsymbol{x})}{∂x_1^2} & \\frac{∂^2f(\\boldsymbol{x})}{{∂x_1}{∂x_2}} & ... & \\frac{∂^2f(\\boldsymbol{x})}{{∂x_1}{∂x_n}} \\\\\n",
    "    \\frac{∂^2f(\\boldsymbol{x})}{{∂x_2}{∂x_1}} & \\frac{∂^2f(\\boldsymbol{x})}{∂x_2^2} & ... & \\frac{∂^2f(\\boldsymbol{x})}{{∂x_2}{∂x_n}}\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "    \\frac{∂^2f(\\boldsymbol{x})}{{∂x_n}{∂x_1}} & \\frac{∂^2f(\\boldsymbol{x})}{{∂x_n}{∂x_2}} & ... & \\frac{∂^2f(\\boldsymbol{x})}{∂x_n^2}\n",
    "    \\end{matrix}\n",
    "\\right] = ∇(∇f(\\boldsymbol{x}))^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二次型应用：最小二乘法\n",
    "# 任意2次多项式都可以用二次型表示\n",
    "# 二次型定义："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 给定矩阵$A∈\\mathbb{R}^{nxn}$，函数\n",
    "$$\\begin{equation*}\n",
    "\\boldsymbol{x}^T\\boldsymbol{A}\\boldsymbol{x} = \\sum_{i=1}^nx_i(\\boldsymbol{A}\\boldsymbol{x})_i = \\sum_{i=1}^nx_i\\left(\\sum_{j=1}^na_{ij}x_j\\right) = \\sum_{i=1}^n\\sum_{j=1}^nx_ix_ja_{ij}\n",
    "\\end{equation*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二次型性质："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 给定对称矩阵$A∈\\mathbb{R}^{nxn}$，如果对于所有$\\boldsymbol{x}∈\\mathbb{R}^n$，有$\\boldsymbol{x}^T\\boldsymbol{Ax}\\ge0$，则为$\\boldsymbol{A}$半正定矩阵，此时特征值$λ(\\boldsymbol{A})\\ge0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果对于所有$\\boldsymbol{x}∈\\mathbb{R}^n$，有$\\boldsymbol{x}^T\\boldsymbol{Ax}>0$，$\\boldsymbol{x}\\ne\\boldsymbol{0}$，则$\\boldsymbol{A}$为正定矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\boldsymbol{A}$为负定矩阵，有$\\boldsymbol{x}^T\\boldsymbol{Ax}<0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 不定矩阵，既可能>0，也可能<0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在实际应用，很多问题都可以转换成二次型，通过矩阵的正负定，来观察其特征值值等情况\n",
    "# 其实也是在多变量的情况将其转换为矩阵，以矩阵的方式来研究二次函数\n",
    "# 再引入高阶泰勒展开，一阶形式就是个二次型，通过求这个二次型的极值来得到原函数在极值点的近似解\n",
    "# 关于二次型的详细参考：https://www.zhihu.com/question/38902714?from=profile_question_card\n",
    "# 在数学中，有如下对应关系："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对称矩阵 $\\Longleftrightarrow$ 二次型矩阵 $\\Longleftrightarrow$ 二次型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高阶求导："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 向量$\\boldsymbol{a}$和$\\boldsymbol{x}$无关，则$∇\\left(\\boldsymbol{a^Tx}\\right) = \\boldsymbol{a}$，$∇^2\\left(\\boldsymbol{a^Tx}\\right) = \\boldsymbol{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 对称矩阵$\\boldsymbol{A}$和$\\boldsymbol{x}$无关，则$∇\\left(\\boldsymbol{x^TAx}\\right) = 2\\boldsymbol{Ax}$，$∇^2\\left(\\boldsymbol{x^TAx}\\right) = 2\\boldsymbol{A}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最小二乘：\n",
    "# 数学本质是最小化系数矩阵张成的向量空间到观察向量的欧式距离\n",
    "# 详情参考：https://www.zhihu.com/question/37031188"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = \\|\\boldsymbol{Ax-b}\\|_2^2 = (\\boldsymbol{Ax-b})(\\boldsymbol{Ax-b}) = \\boldsymbol{A^Tx^TAx-A^Tx^Tb-Axb^T+b^Tb}$，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因$\\boldsymbol{A^Tx^Tb}$和$\\boldsymbol{Axb^T}$乘积都为常数，而$\\boldsymbol{A^Tx^Tb}$ = $\\boldsymbol{Axb^T}$，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上式进一步化简得：$\\boldsymbol{A^Tx^TAx-2Axb^T+b^Tb}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对$\\boldsymbol{x}$求导，$∇f(\\boldsymbol{x})=2\\boldsymbol{A^TAx-A^Tb}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最小二乘的向量矩阵二次型表达式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(\\boldsymbol{x})=\\boldsymbol{x^TAx+2b^Tx}+c$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 泰勒级数：是一种对高次曲线的近似拟合，特别是在极值点的近似求解\n",
    "# 当n足够大，泰勒级数就可以近似任何函数，不仅如sin(x) e^x等 是一把幂函数的近似线性组合表达\n",
    "# 泰勒级数展开(标量和向量)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于泰勒级数的一些tips：\n",
    "\n",
    "作者：George Chuy\n",
    "链接：https://www.zhihu.com/question/28486177/answer/129933751\n",
    "来源：知乎\n",
    "著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n",
    "\n",
    "问：泰勒级数在干嘛？答：做近似。你随便给我一个函数，它的表达式可能非常复杂，甚至没有表达式。但是幂函数们有很简单的表达式，性质也很清楚。所以人就会想要，把简单的东西组合起来（线性组合），来代替一个复杂的东西。\n",
    "\n",
    "问：收敛域是干嘛的？答：要是你只对一个函数在很小的一个自变量范围（领域）内的性质感兴趣，你就可以用泰勒级数这个工具。其实泰勒级数只是一套更复杂的工具（函数空间）的一种功能，一种应用而已。\n",
    "\n",
    "问：函数空间是什么东西？和泰勒级数有什么不一样？答：泰勒级数只关心曲线的一小段儿，但是如果你有了函数空间这个东西，你就可以对整段曲线，做一个近似或者代替。\n",
    "\n",
    "问：为什么泰勒级数一定要用幂函数来近似或者代替呢?答：没错，你可以用别的函数，像正弦函数（傅里叶级数）或者是脉冲函数（只在一个点不为零，别的地方都为零）。\n",
    "\n",
    "问：什么叫函数空间？答：人用其函数空间来，觉得他跟矢量空间很像，所以给他了一个空间的名字。在矢量空间里，你随便给一个矢量，我只要有一组完备的基本矢量，就能够完全精确地来代替他。（不再是近似哦。）在函数空间里，要是我们手里有个把的幂函数，我们要是派给他们每人一个合适的系数，就能够让他们的组合，在一个范围里边儿，非常像你最早给我的那个函数。其实呢，要是你把所有的幂函数都给我的话，他们就能够表示随便一个什么函数。也就是说，如果他们是完备的，他们就不再是近似了而是代替，是在整个曲线上没有误差的描述，不再是在曲线的一小段上的描述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说明：这边令$\\boldsymbol{g^T(x_k)}=f'(\\boldsymbol{x_k})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 输入为标量的泰勒级数展开\n",
    "$$f(x_k+δ) ≈ f(x_k) + f'(x_k)δ + \\frac{f''(x_k)δ^2}{2}+ ... + \\frac{f^k(x_k)δ^k}{k!} + ...$$\n",
    "\n",
    "> 说明：$x = x_k + \\delta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 输入为向量的泰勒级数展开\n",
    "$$f(\\boldsymbol{x_k+δ}) ≈ f(\\boldsymbol{x_k}) + \\boldsymbol{g^T(x_k)δ} + \\frac{\\boldsymbol{δ^TH(x_k)δ}}{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 泰勒级数和极值\n",
    "# 泰勒级数：用泰勒的多项展开来近似高次函数在极值点的近似解\n",
    "# 详细参考：https://www.zhihu.com/question/21149770\n",
    "# 标量情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 输入为标量的泰勒级数展开\n",
    "$$f(x_k+δ) ≈ f(x_k) + f'(x_k)δ + \\frac{f''(x_k)δ^2}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 严格局部极小点有：$f(x_k+δ) \\ge f(x_k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如下图为凸函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(-40,1600,'$f(x_k-δ)>f(x_k)$')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.linspace(-50,50,100)\n",
    "y = x**2\n",
    "plt.plot(x,y,c='r')\n",
    "plt.axvline(x=0,linestyle='--')\n",
    "plt.axvline(x=40,linestyle='--')\n",
    "plt.axvline(x=-40,linestyle='--')\n",
    "plt.axhline(y=40**2,linestyle='--')\n",
    "plt.axhline(y=0,linestyle='--')\n",
    "plt.text(0,0,s='$x_k$',fontsize=12)\n",
    "plt.text(40,0,s='$δ$',fontsize=12)\n",
    "plt.text(-40,0,s='$-δ$',fontsize=12)\n",
    "plt.text(40,1600,s='$f(x_k+δ)>f(x_k)$',fontsize=12)\n",
    "plt.text(-40,1600,s='$f(x_k-δ)>f(x_k)$',fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解释：因要求f(xk+δ)≥f(xk)，而δ可正可负，那只能f′(xk)=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 称满足$f'(x_k)$ = 0的点为平稳点(候选点)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：此处候选点有极大值、极小值和鞍点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 函数在$x_k$有严格局部极小值条件为$f'(x_k)'$ = 0且$f''(x_k)$ > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解释：因f′(xk)=0了，上述泰勒级数要满足条件，只能f″(xk) > 0\n",
    "# 因要求满足f(x_k+δ)>=f(x_k)，而f′(xk)=0，要泰勒近似条件就必须f″(xk)>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 输入为向量的泰勒级数展开\n",
    "$$f(\\boldsymbol{x_k+δ}) ≈ f(\\boldsymbol{x_k}) + \\boldsymbol{g^T(x_k)δ} + \\frac{\\boldsymbol{δ^TH(x_k)δ}}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 称满足$\\boldsymbol{g^T(x_k)}$ = 0的点为平稳点(候选点)，此时如果有$\\boldsymbol{H(x_k)} \\succ 0$，$x_k$为一严格局部极小点(反之，严格局部最大点) 如果$\\boldsymbol{H(x_k)}$为不定矩阵，则是一个鞍点 既不是极大值 也不是极小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断严格局部极小点的条件：\n",
    "# 一阶导=0，二阶导大于0\n",
    "# 二阶导小于0为极大点\n",
    "# 二阶导等于0，则需要三阶导来判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度为0求解的局限性\n",
    "# 原函数如果很复杂，求导函数必然也很复杂，要求解极值点，固然是很困难的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如计算：$f(x)=x^4+sin(x^2)-ln(x)e^x+7$的导数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f'(x) = 4x^3 + 2xcos(x^2) - \\frac{e^x}{x} - ln(x)e^x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令其$f'(x)$ = 0，显然很难求得解$x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：此时，就可以选择迭代法求解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2、迭代法求解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 迭代法的基本结构(最小化f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、选择一个初始点，设置一个容忍度$\\epsilon$(循环停止条件)，计数$k$ = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：初始点的选择会影响结果，不同的初始点会选取不同的下降路径，应尽量多选择几个初始点\n",
    "# 就比如说我们在GMM中，使用EM算法计算模型参数，选择初始值建议先使用k-means找到质心μ和对应的Σ，还有π，会加快迭代速度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、决定搜索方向$\\boldsymbol{d_k}$，使得函数下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、决定步长$α_k$使得$f(\\boldsymbol{x_k}+α_k\\boldsymbol{d_k})$对于$α_k \\ge 0$最小化，构建$\\boldsymbol{x_{k+1}=x_k}+α_k\\boldsymbol{d_k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：步长αk的选择，可以自定义初始值，一般为0.01,0.1大小，也可以通过利用偏导求极值得到"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4、如果$\\|\\boldsymbol{d_k}\\|$ < $\\epsilon$，则停止输出解$\\boldsymbol{x_{k+1}}$；否则继续重复迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解释：为什么dk的模长作为停止条件？也就是梯度的模长\n",
    "# 这边的dk是下降的方向，也是梯度的反方向，如果是凸函数，会下降到||dk||=0的极值点\n",
    "# 这边设置一个停止条件，一般是求解局部极值，以防止诸如鞍点的情况，这种情况会长时间在某个方向上游荡，会耗费时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ① 梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\boldsymbol{d_k} = -\\boldsymbol{g(x_k)}$，思考为什么这么取？\n",
    "$$f(\\boldsymbol{x_k+d_k})≈f(\\boldsymbol{x_k})+\\boldsymbol{g^T(x_k)d_k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 要使$f(\\boldsymbol{x_k+d_k})\\downarrow$且要最快，梯度为正，最小值为向量$0$，所以只能$f(\\boldsymbol{x_k})$加个负数，且为负梯度方向$\\boldsymbol{g(x_k)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 我们可以从两个向量的内积考虑，$\\boldsymbol{a.b} = \\boldsymbol{a^Tb} = \\|\\boldsymbol{a}\\|\\|\\boldsymbol{b}\\|cos(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解释：两向量内积的最大值，为θ为180°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ② 牛顿法\n",
    "# 说明：梯度下降法使用的一阶导项，舍去了二阶导项，其实二阶导项还是占据很大比重的\n",
    "# 另外使用二阶导，下降速度更快，为什么这么说呢？梯度下降只是在保留一阶项时求得的最优解，并不是二阶导的最优值\n",
    "# 或者说可以这么理解，一阶导是斜率，二阶导是斜率的变化率，自然二阶导直接蹦着最佳斜率去的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 方向选取$\\boldsymbol{d_k} = - \\boldsymbol{H^{-1}}(\\boldsymbol{x_k})\\boldsymbol{g}(\\boldsymbol{x_k})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 方向选取依据\n",
    "$$f(\\boldsymbol{x_k+d_k}) ≈ f(\\boldsymbol{x_k}) + \\boldsymbol{g^T(x_k)d_k} + \\frac{\\boldsymbol{d_k^TH(x_k)d_k}}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 令$\\frac{∂f(\\boldsymbol{x_k+d_k})}{∂\\boldsymbol{d_k}} = \\boldsymbol{0} \\Rightarrow \\boldsymbol{g}(\\boldsymbol{x_k})\n",
    "+\\boldsymbol{H}(\\boldsymbol{x_k})\\boldsymbol{d_k} = \\boldsymbol{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 若Hessian矩阵正定，则有$\\boldsymbol{d_k} = - \\boldsymbol{H^{-1}}(\\boldsymbol{x_k})\\boldsymbol{g}(\\boldsymbol{x_k})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解释：同时要求Hessian矩阵可逆，并且正定\n",
    "# 正定矩阵一定可逆：它的特征值分解都为正，行列式也就不可能等于0\n",
    "# 要求Hessian矩阵正定：将dk=−H−1(xk)g(xk)带入原式中，可得到只有当Hessian为正定，也就是>0时，才能保证整个式子是下降的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 牛顿法关键点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 实际工程中Hessian矩阵$H$很难求，$H^{-1}$更加难求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：采用牛顿法，需要消耗大量的内存和计算量，一般在特征值较少，梯度法失效，可以减少一定的迭代时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一种情况，Hessian矩阵若不是正定矩阵，可对Hessian矩阵进行修正："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol{H}(\\boldsymbol{x_k})+\\boldsymbol{E}$，典型的方法$\\boldsymbol{E} = δ\\boldsymbol{I}$，δ>0很小。思考为什么这么取？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解释：若H(xk)非正定矩阵，那最直接的想法就是加上一个正定矩阵，使得H(xk)变得正定，最简单的方法自然是加上一个δ>0的倍的单位阵I\n",
    "# 也可以使用迭代法取修正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 介于存在Hessian矩阵难求，其逆更难求，以及Hessian矩阵会出现非正定的情况，出现了拟牛顿法的选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ③ 拟牛顿法\n",
    "# 核心思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 统一深度下降法和牛顿法：\n",
    "$$\\boldsymbol{d_k} = -\\boldsymbol{S_kg_k}$$\n",
    "其中$\\boldsymbol{S_k} = \n",
    "\\begin{cases} \n",
    "      \\boldsymbol{I} & steepest \\\\\n",
    "      \\boldsymbol{H_k^{-1}} & Newton \\\\\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 不直接求$\\boldsymbol{H_k^{-1}}$，尝试用一正定矩阵逼近$\\boldsymbol{H_k^{-1}}$(一阶的量慢慢近似二阶的量)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 定义$\\boldsymbol{δ_k=x_{k+1}-x_k}$，$\\boldsymbol{γ_k=g_{k+1}-g_k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 需要$\\boldsymbol{S_{k+1}γ_k=δ_k}$，为什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解释如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol{S_{k+1}} = \\large\\boldsymbol{\\frac{δ_k}{γ_k}} = \\large\\boldsymbol{\\frac{x_{k+1}-x_k}{g_{k+1}-g_k}}$，\n",
    "而因$\\large\\boldsymbol{\\frac{g_{k+1}-g_k}{x_{k+1}-x_k}}$恰为二阶导数的极限表示形式，继而有$\\large\\boldsymbol{\\frac{x_{k+1}-x_k}{g_{k+1}-g_k}}$为二阶导数的逆，由此得到近似矩阵的逆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二阶导数的极限表示形式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\lim_{n \\to \\infty}\n",
    "\\sum_{x=1}^n \\large\\frac{f'(x+δ)-f'(x)}{δ}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 只有$\\boldsymbol{δ_k}$和$\\boldsymbol{γ_k}$，是无法计算出$\\boldsymbol{S_{k+1}}$的，继续用迭代的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：δk和γk都是向量，而Sk+1是矩阵，是无法求得的\n",
    "# 我们就用迭代的方法不断近似这个Hessian矩阵的逆\n",
    "# 常用的迭代方法有DFP和BFGS\n",
    "# DFP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 给定初始$\\boldsymbol{S_0}=\\boldsymbol{I}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\boldsymbol{S_{k+1}} = \\boldsymbol{S_k} + \\triangle\\boldsymbol{S_k}$，$k=0,1,...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\triangle\\boldsymbol{S_k}$ = $α\\boldsymbol{uu^T}$ + $β\\boldsymbol{vv^T}$，因此\n",
    "$$\\boldsymbol{S_{k+1}} = \\boldsymbol{S_k} + α\\boldsymbol{uu^T} + β\\boldsymbol{vv^T}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：为什么要如此选择△Sk = αuuT + βvvT，确保每次迭代更新Sk+1都是正定的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 两边乘以$\\boldsymbol{γ_k}$，有$\\boldsymbol{δ_k} = \\boldsymbol{S_kγ_k} + \\boldsymbol{\\underbrace{\\left(αu^Tγ_k\\right)}_{1}u} + \\boldsymbol{\\underbrace{\\left(βv^Tγ_k\\right)}_{-1}v}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# αuTγk和βvTγk都是两向量内积的形式，为一个常数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 令$\\boldsymbol{(αu^Tγ_k)} = 1$，$\\boldsymbol{(βv^Tγ_k)} = -1$，上式得$\\boldsymbol{S_kγ_k} + \\boldsymbol{u} - \\boldsymbol{v}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：u和v都是未知量，我们可以调整αuTγk和βvTγk的值，令其(αuTγk)=1, (βvTγk)v)=−1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 由此解出$α = \\frac{1}{\\boldsymbol{u^Tγ_k}}$，$β = -\\frac{1}{\\boldsymbol{v^Tγ_k}}$，且有$\\boldsymbol{u} - \\boldsymbol{v} = \\boldsymbol{δ_k} - \\boldsymbol{S_kγ_k}$，通过解方程可得$\\boldsymbol{u}$和$\\boldsymbol{v}$，从而最终解得："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DFP更新公式\n",
    "$$\\boldsymbol{S_{k+1}} = \\boldsymbol{S_k} + \\boldsymbol{\\frac{δ_kδ_k^T}{δ_k^Tγ_k} - \\frac{S_kγ_kγ_k^TS_k}{γ_k^TS_kγ_k}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：\n",
    "# 几点注意：1、整个迭代过程中需要保证Sk+1始终是正定矩阵；2、不断调整αuTγk和βvTγk的值，使得满足条件 u,v,γk都是向量\n",
    "# 为什么说是用一阶导数的量在近似二阶导数的量？\n",
    "# 从上述更新公式中也可以看出γk涉及到gk的一阶导，另外本身在设定时，采用了二阶导极限的逆矩阵近似\n",
    "# 每次dk方向的决定都来自于上次的-Skgk\n",
    "# Sk = I时，就是梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFGS\n",
    "# 对于δk和γk的选择不同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BFGS：$\\boldsymbol{S_0}=\\boldsymbol{I}$\n",
    "$$\\boldsymbol{S_{k+1}} = \\boldsymbol{S_k} + \\left(1 + \\boldsymbol{\\frac{γ_k^TS_kγ_k}{δ_k^Tγ_k}}\\right)\\boldsymbol{\\frac{δ_kδ_k^T}{δ_k^Tγ_k}} - \\boldsymbol{\\frac{δ_kγ_k^TS_k + S_kγ_kδ_k^T}{δ_k^Tγ_k}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DFP和BFGS在算法效率上差不多，只不过是选择使用BFGS的较多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步长α求取，实际工程中每次迭代使用设定的固定步长"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 每次迭代固定步长，实际中最常用，例如$α_k$ = $α$ = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 求导。例如$f(\\boldsymbol{x})=\\boldsymbol{x^TAx+2b^Tx}+c$，需要解$min_{α \\ge 0} f(\\boldsymbol{x}+α\\boldsymbol{d})$则$h(α) = f(\\boldsymbol{x}+α\\boldsymbol{d})$，则有$\\frac{\\partial{h(α)}}{\\partialα} = 0 \\Rightarrow α = -\\boldsymbol{\\frac{d^T\\nabla{f(x)}}{2d^TAd}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "链式法则对$α$求偏导，设$\\boldsymbol{u} = \\boldsymbol{x} + α\\boldsymbol{d}$, 则有$\\boldsymbol{f(u)} = \\boldsymbol{u^TAu} + \\boldsymbol{2b^Tu} + c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol{f'(u)} = \\boldsymbol{\\frac{∂h(α)}{∂u}} = 2\\boldsymbol{Au} + 2\\boldsymbol{b^T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol{u'(α)} = \\boldsymbol{\\frac{∂u}{∂α}} = \\boldsymbol{d^T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol{f'(x)} = \\boldsymbol{\\frac{df}{dx}} = 2\\boldsymbol{(Ax + b^T)} \\Rightarrow 2\\boldsymbol{Ax} = \\boldsymbol{f'(x)} -2\\boldsymbol{b^T} \\Rightarrow \\boldsymbol{x} = \\frac{\\boldsymbol{f'(x) - 2b^T}}{2A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol{h'(α)} = \\boldsymbol{\\frac{∂h(α)}{∂u}}\\boldsymbol{\\frac{∂u}{∂α}} = 2\\boldsymbol{(Ax + b^T)}\\boldsymbol{d^T} = 2\\boldsymbol{d^T}\\boldsymbol{[A(x+αd) + b^T]}\n",
    "\t\t    = 2\\boldsymbol{d^T}[\\boldsymbol{A}(\\frac{(f'(x) - 2b^T)}{2A}+α\\boldsymbol{d}) + \\boldsymbol{b^T}]\n",
    "\t\t    = 2\\boldsymbol{d^T}(\\boldsymbol{\\frac{f'(x)}{2} - b^T + b^T} + α\\boldsymbol{dA})\n",
    "\t\t    = 2\\boldsymbol{d^T}(\\boldsymbol{\\frac{f'(x)}{2}} + α\\boldsymbol{dA})\n",
    "\t\t    = \\boldsymbol{d^Tf'(x)} + 2α\\boldsymbol{d^TAd} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "则求得 $α = -\\frac{\\boldsymbol{d^Tf'(x)}}{2\\boldsymbol{d^TAd}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 不精确的线搜索和Armijo条件\n",
    "\n",
    "$$f(\\boldsymbol{x_k}+α\\boldsymbol{d_k}) < f(\\boldsymbol{x_k}) + c_1α\\boldsymbol{g^T(x_k)d_k}$$\n",
    "\n",
    "设置$c_1 = 10^{-4}$。先从$α= 1$开始搜，如果Armijo条件不满足，设置一回调因子$β∈(0,1)$，将步长下调至$α = βα$。如果还不满足，继续回调，从而保证步长不至于太小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性回归模型求解\n",
    "# 求解方法1：利用梯度等于0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 试图学习：$f(x) = \\boldsymbol{w}^Tx + b$ 使得$f(\\boldsymbol{x}^{(i)})≈y^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 令未知$\\boldsymbol{\\bar{w}} = \\left[\\begin{matrix}\\boldsymbol{w}\\\\b\\end{matrix}\\right]$，已知$\\boldsymbol{X} = \n",
    "\\left[\n",
    "    \\begin{matrix}\n",
    "    \\boldsymbol{x}^{(1)T} & 1 \\\\\n",
    "    \\vdots & \\vdots \\\\\n",
    "    \\boldsymbol{x}^{(N)T} & 1\n",
    "    \\end{matrix}\n",
    "\\right]_{Nx(d+1)}$，\n",
    "则有$$\\boldsymbol{y≈X\\bar{w}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 损失函数$\\|\\boldsymbol{y-X\\bar{w}}\\|_2^2$，求解$$min\\hspace{0.1cm}\\|\\boldsymbol{y-X\\bar{w}}\\|_2^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：上述式子很明显是一个二次型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $g(\\boldsymbol{\\bar{w}}) = 0 \\Rightarrow 2\\boldsymbol{X^T(X\\bar{w}-y)} = 0 \\Rightarrow \\boldsymbol{\\bar{w}^*} = \\boldsymbol{(X^TX)^{-1}X^Ty}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对上式使用链式法则求导，令$\\boldsymbol{u=X\\bar{w}−y}$，得到$min\\hspace{0.1cm}\\|\\boldsymbol{u}\\|_2^2$，对$\\bar{w}$求偏导，\n",
    "$g(\\boldsymbol{\\bar{w}}) = \\boldsymbol{\\frac{∂g(\\bar{w})}{∂u}}\\boldsymbol{\\frac{∂u}{∂\\bar{w}}} = 2\\boldsymbol{uX^T} = 2\\boldsymbol{X^T(X\\bar{w}−y)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 适当时，需要加入正则化项，防止过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求解方法2：梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 梯度下降法 $$g(\\boldsymbol{\\bar{w}}) = 2\\boldsymbol{X^T(X\\bar{w}−y)} = 2\\sum_{i=1}^N\\boldsymbol{x^{(i)}\\left(w^Tx^{(i)}-y^{(i)}\\right)}$$\n",
    "$$\\boldsymbol{\\bar{w}}\\leftarrow\\boldsymbol{\\bar{w}}-αg(\\boldsymbol{\\bar{w}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 随机梯度下降法SGD(实际中很有用)\n",
    "$$\\Big \\{i=1:N,2\\boldsymbol{x^{(i)}\\left(w^Tx^{(i)}-y^{(i)}\\right)}\\Big \\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 说明：因为批量梯度下降法，每次迭代都需要遍历所有样本，小样本量还好，大样本数据收敛时间会变长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为什么说牛顿法比梯度下降法下降速度更快？\n",
    "# 1、梯度下降法为一阶导数，牛顿法为二阶导数，几何意义上，二次函数近似更准确，一阶导数是梯度，二阶导数是梯度的梯度；\n",
    "# 2、梯度下降是保证每次都是朝着下降的方向走的，能够先找到局部最优点，而对于非凸函数，先使用梯度下降就比较好，保证能够找到最优点，不一定是全局的，而对于凸函数，使用牛顿法，就一步到位了；\n",
    "# 3、牛顿法是要求hessian矩阵的，在高次项中，也就是说维数很高时，求hessian矩阵是很难的，它的逆就更难了，非常耗内存，计算量也大\n",
    "# 4、梯度下降不容易收敛，会在极值点附近震荡，牛顿法则比较容易收敛\n",
    "# 5、实际工程中，可能会先使用梯度下降，找到一个局部点，再使用牛顿法，或者直接使用拟牛顿法\n",
    "# 参考：https://www.zhihu.com/question/19723347"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在scipy.optimize.minimize模块中有多种算法选择\n",
    "# 而无约束梯度分析和迭代法都不仅限于线性回归问题\n",
    "# 模块sklearn.linear_model.LinearRegression并未指明使用何种方法求解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_numer=1,learning_rate=0.103175,norm_grad=3.237473,fun_val=2.539683\n",
      "iter_numer=2,learning_rate=0.433333,norm_grad=2.158315,fun_val=0.268749\n",
      "iter_numer=3,learning_rate=0.103175,norm_grad=0.342590,fun_val=0.028439\n",
      "iter_numer=4,learning_rate=0.433333,norm_grad=0.228393,fun_val=0.003009\n",
      "iter_numer=5,learning_rate=0.103175,norm_grad=0.036253,fun_val=0.000318\n",
      "iter_numer=6,learning_rate=0.433333,norm_grad=0.024169,fun_val=0.000034\n",
      "iter_numer=7,learning_rate=0.103175,norm_grad=0.003836,fun_val=0.000004\n",
      "iter_numer=8,learning_rate=0.433333,norm_grad=0.002558,fun_val=0.000000\n",
      "iter_numer=9,learning_rate=0.103175,norm_grad=0.000406,fun_val=0.000000\n",
      "iter_numer=10,learning_rate=0.433333,norm_grad=0.000271,fun_val=0.000000\n",
      "iter_numer=11,learning_rate=0.103175,norm_grad=0.000043,fun_val=0.000000\n",
      "iter_numer=12,learning_rate=0.433333,norm_grad=0.000029,fun_val=0.000000\n",
      "iter_numer=13,learning_rate=0.103175,norm_grad=0.000005,fun_val=0.000000\n",
      "iter_numer=14,learning_rate=0.433333,norm_grad=0.000003,fun_val=0.000000\n",
      "iter_numer=15,learning_rate=0.103175,norm_grad=0.000000,fun_val=0.000000\n",
      "iter_numer=16,learning_rate=0.433333,norm_grad=0.000000,fun_val=0.000000\n",
      "iter_numer=17,learning_rate=0.103175,norm_grad=0.000000,fun_val=0.000000\n"
     ]
    }
   ],
   "source": [
    "# 梯度下降算法：\n",
    "# 说明：learning_rate为学习率或为步长，norm_grad为搜索方向的模长，fun_val为每次迭代得到的函数值可以理解为损失函数每次迭代的值\n",
    "# 这边矩阵A，截距b和初始点x0，还有停止条件epsilon为选定值\n",
    "# 这边的损失函数被写成了二次型形式，因任何二次多项式都可以用二次型来表示，如线性回归的最小均方误差\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def gradient_method_quadratic(*args):\n",
    "    x=x0\n",
    "    i=0\n",
    "    grad=2*(A.dot(x)+b)\n",
    "    while np.linalg.norm(grad)>epsilon:\n",
    "        i=i+1\n",
    "        t=np.square(np.linalg.norm(grad))/(2*grad.T.dot(A).dot(grad))\n",
    "        x=x-t*grad\n",
    "        grad=2*(A.dot(x)+b)\n",
    "        fun_val=x.T.dot(A).dot(x)+2*b.T.dot(x)\n",
    "        print('iter_numer=%i,learning_rate=%2.6f,norm_grad=%2.6f,fun_val=%2.6f' %(i,t,np.linalg.norm(grad),fun_val))\n",
    "        \n",
    "if __name__ == '__main__':       \n",
    "    A=np.array([[1,0],[0,5]])\n",
    "    b=np.array([0,0])\n",
    "    x0=np.array([2,2])\n",
    "    epsilon=np.exp(-16)\n",
    "    gradient_method_quadratic(A,b,x0,epsilon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
